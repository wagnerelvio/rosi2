# -*- coding: utf-8 -*-
"""dscan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KAexdkW_nDMcWzuTxLU_zSANzCNR6mCh
"""

from google.colab import drive
drive.mount('/content/drive')

from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, davies_bouldin_score
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Carregando o conjunto de dados Online Shoppers Purchasing Intention Dataset
path = "/content/drive/MyDrive/Colab_Notebooks/ml_2naousar/winequality-red.csv"
df = pd.read_csv(path, sep=";")

###
print(df.isnull().sum())

###
#visualisar as variáveis
df.hist(figsize=(12, 10))
plt.show()

df.boxplot(figsize=(12, 6))
plt.show()

sns.pairplot(df) # Matriz de dispersão (pode ser lento para muitos atributos)
plt.show()

####
# Análise de outliers (exemplo com IQR)
def detect_outliers_iqr(data):
    q1, q3 = np.percentile(data, [25, 75])
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    outliers = data[(data < lower_bound) | (data > upper_bound)]
    return outliers

for col in df.columns:
    outliers = detect_outliers_iqr(df[col])
    print(f"Outliers in {col}: {outliers}")

# Padronizar os dados
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df.drop(columns=["quality"]))

# Reduzir a dimensionalidade com PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Testar diferentes valores de eps e min_samples
eps_values = [0.3, 0.4, 0.5, 0.6]
min_samples_values = [3, 5, 10]
best_eps, best_min_samples = None, None
best_silhouette = -1

for eps in eps_values:
    for min_samples in min_samples_values:
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = dbscan.fit_predict(X_pca)

        # Excluir ruído (-1) para cálculo dos índices
        if len(set(labels)) > 1 and -1 in labels:
            labels_filtered = labels[labels != -1]
            X_filtered = X_pca[labels != -1]
        else:
            labels_filtered = labels
            X_filtered = X_pca

        if len(set(labels_filtered)) > 1:
            silhouette_avg = silhouette_score(X_filtered, labels_filtered)
            davies_bouldin = davies_bouldin_score(X_filtered, labels_filtered)
            print(f"eps: {eps}, min_samples: {min_samples} -> Silhouette: {silhouette_avg:.4f}, Davies-Bouldin: {davies_bouldin:.4f}")

            if silhouette_avg > best_silhouette:
                best_silhouette = silhouette_avg
                best_eps, best_min_samples = eps, min_samples

print(f"Melhores parâmetros: eps={best_eps}, min_samples={best_min_samples}")

# Aplicar DBSCAN com os melhores parâmetros encontrados
dbscan_best = DBSCAN(eps=best_eps, min_samples=best_min_samples)
labels_best = dbscan_best.fit_predict(X_pca)

#Testar diferentes valores de eps e min_samples para tentar obter 6 clusters
eps_values = np.arange(0.1, 1.0, 0.05)  # Testando valores menores de eps
min_samples_values = range(2, 15)  # Aumentando a variação de min_samples

best_eps, best_min_samples = None, None
best_silhouette = -1
best_num_clusters = 0

for eps in eps_values:
    for min_samples in min_samples_values:
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = dbscan.fit_predict(X_pca)

        # Excluir ruído (-1) para cálculo dos índices
        unique_clusters = set(labels)
        if -1 in unique_clusters:
            unique_clusters.remove(-1)
        num_clusters = len(unique_clusters)

        if num_clusters >= 2:
            silhouette_avg = silhouette_score(X_pca[labels != -1], labels[labels != -1])
            davies_bouldin = davies_bouldin_score(X_pca[labels != -1], labels[labels != -1])
            print(f"eps: {eps:.2f}, min_samples: {min_samples} -> Clusters: {num_clusters}, Silhouette: {silhouette_avg:.4f}, Davies-Bouldin: {davies_bouldin:.4f}")

            # Buscar uma configuração próxima de 6 clusters
            if abs(num_clusters - 6) < abs(best_num_clusters - 6) or best_eps is None:
                best_eps, best_min_samples = eps, min_samples
                best_num_clusters = num_clusters
                best_silhouette = silhouette_avg

print(f"Melhores parâmetros encontrados: eps={best_eps}, min_samples={best_min_samples}, Clusters={best_num_clusters}")

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

# Definir o número de clusters desejado
n_clusters = 6

# Aplicar K-Means
kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
labels_kmeans = kmeans.fit_predict(X_pca)

# Adicionar os rótulos dos clusters ao dataframe original
df_clusters_kmeans = df.copy()
df_clusters_kmeans["Cluster"] = labels_kmeans

####pontos de ruido
noise_data = df[df['DBSCAN_Cluster'] == -1]
print("\nPontos de Ruído:")
print(noise_data.describe())

# Criar um DataFrame para análise dos clusters

# Agrupar por cluster e calcular estatísticas
cluster_summary = df_clusters_kmeans.groupby('Cluster').mean()
cluster_summary['count'] = df_clusters_kmeans['Cluster'].value_counts()

# Exibir as principais diferenças entre os clusters
print(cluster_summary)

###descritiva dos clusteres
df['DBSCAN_Cluster'] = labels_best
for cluster_label in set(labels_best):
    cluster_data = df[df['DBSCAN_Cluster'] == cluster_label]
    print(f"Cluster {cluster_label}:")
    print(cluster_data.describe())

# Plotar os clusters encontrados
plt.figure(figsize=(10, 6))
colors = sns.color_palette("tab10", n_colors=n_clusters)

for label in range(n_clusters):
    cluster_points = X_pca[labels_kmeans == label]
    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=[colors[label]], alpha=0.7, label=f"Cluster {label}")

# Adicionar detalhes ao gráfico
plt.xlabel("PC1 (Principal Componente 1)")
plt.ylabel("PC2 (Principal Componente 2)")
plt.title("Clusters de Vinhos (K-Means, 6 Grupos)")
plt.legend()
plt.show()

# Exibir a média das características por cluster
clusters_summary_kmeans = df_clusters_kmeans.groupby("Cluster").mean()
clusters_summary_kmeans

##talvez nao seja necessario esse passo aqui
import matplotlib.pyplot as plt
import seaborn as sns

# Remover a contagem para evitar impacto no gráfico
cluster_summary_plot = cluster_summary.drop(columns=['count'])

# Criar gráfico de barras para visualizar as médias dos atributos por cluster
plt.figure(figsize=(14, 8))
cluster_summary_plot.T.plot(kind='bar', figsize=(14, 6), colormap='tab10', width=0.8)

plt.title('Comparação dos Atributos Médios em Cada Cluster')
plt.xlabel('Atributos do Vinho')
plt.ylabel('Média Normalizada')
plt.xticks(rotation=45)
plt.legend(title="Cluster", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Normalizar os dados para facilitar a visualização
cluster_summary_norm = (cluster_summary_plot - cluster_summary_plot.min()) / (cluster_summary_plot.max() - cluster_summary_plot.min())

# Criar heatmap para destacar as diferenças entre os clusters
plt.figure(figsize=(12, 6))
sns.heatmap(cluster_summary_norm.T, annot=True, cmap="coolwarm", linewidths=0.5, fmt=".2f")

plt.title('Heatmap das Características Predominantes por Cluster')
plt.xlabel('Clusters')
plt.ylabel('Atributos do Vinho')
plt.xticks(rotation=0)
plt.yticks(rotation=0)
plt.show()

"""**Cluster 0**

Alta acidez fixa e alto teor alcoólico

Boa qualidade geral

Níveis médios de sulfatos

**Cluster 1**

Alta acidez volátil e alto teor alcoólico

Baixo teor de ácido cítrico e sulfatos

Qualidade intermediária

Alto ph

**Cluster 2**

Densidade intermediária e acidez fixa moderada

Teor alcoólico mais baixo

Qualidade mediana

**Cluster 3**

pH elevado e alto teor de açúcar residual

Densidade relativamente alta

Qualidade mais baixa comparada aos outros grupos

**Cluster 4**

Alta acidez fixa e alto teor de ácido cítrico

Menor acidez volátil

Maior concentração de sulfatos

**Cluster 5**

Maior teor de enxofre livre e total

Densidade elevada e alta acidez volátil

Teor alcoólico relativamente baixo
"""